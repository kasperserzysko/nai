BASED ON 100 EXAMPLES

ROUGE SCORES:

Falconsai/text_summarization: {'rouge1': 0.2575378291267752, 'rouge2': 0.07325487567155982, 'rougeL': 0.1948950337024763, 'rougeLsum': 0.1946892825689503}
facebook/bart-large-cnn: {'rouge1': 0.3105353073328816, 'rouge2': 0.1062044193647649, 'rougeL': 0.23104099082274993, 'rougeLsum': 0.23096606552938775}
csebuetnlp/mT5_multilingual_XLSum: {'rouge1': 0.17032686598259938, 'rouge2': 0.02129226139325538, 'rougeL': 0.12891541417822328, 'rougeLsum': 0.12832759527170845}

BERT SCORES (AVG):
Falconsai/text_summarization: PRECISION: 0.8186914336681366, F1: 0.8496503043174743, RECALL: 0.8836200833320618
facebook/bart-large-cnn: PRECISION: 0.841158407330513, F1: 0.8674615824222565, RECALL: 0.8961014044284821
csebuetnlp/mT5_multilingual_XLSum: PRECISION: 0.8501370990276337, F1: 0.8525621253252029, RECALL: 0.8552090936899185